{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**Mount Drive**"
      ],
      "metadata": {
        "id": "Bx2NbhDomMSn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AyoOSrvg3xw",
        "outputId": "2decc11f-4d11-496a-ac27-340cd8c099f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "GPU Available:  True\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"GPU Available: \", tf.test.is_gpu_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Split 90% Train, 5% Val, 5% Test**"
      ],
      "metadata": {
        "id": "Noff5MmZmSHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Tentukan path dataset\n",
        "dataset_path = \"/content/drive/MyDrive/Dataset 224x224\"\n",
        "\n",
        "# Tentukan path output untuk menyimpan dataset terbagi\n",
        "output_path = \"/content/drive/MyDrive/DatasetSplit\"\n",
        "\n",
        "# Buat direktori output jika belum ada\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "# Membaca nama file dari setiap kelas\n",
        "normal_files = os.listdir(os.path.join(dataset_path, \"Normal\"))\n",
        "pneumonia_files = os.listdir(os.path.join(dataset_path, \"Pneumonia\"))\n",
        "mass_files = os.listdir(os.path.join(dataset_path, \"Mass\"))\n",
        "nodule_files = os.listdir(os.path.join(dataset_path, \"Nodule\"))\n",
        "tuberculosis_files = os.listdir(os.path.join(dataset_path, \"Tuberculosis\"))\n",
        "\n",
        "# Menggabungkan semua kategori\n",
        "all_files = {\n",
        "    \"Normal\": normal_files,\n",
        "    \"Pneumonia\": pneumonia_files,\n",
        "    \"Mass\": mass_files,\n",
        "    \"Nodule\": nodule_files,\n",
        "    \"Tuberculosis\": tuberculosis_files\n",
        "}\n",
        "\n",
        "# Mengacak urutan file di setiap kategori\n",
        "for category in all_files:\n",
        "    random.shuffle(all_files[category])\n",
        "\n",
        "# Tentukan rasio pembagian dataset\n",
        "train_ratio = 0.9\n",
        "validation_ratio = 0.05\n",
        "test_ratio = 0.05\n",
        "\n",
        "# Memisahkan dataset menjadi bagian pelatihan, validasi, dan pengujian\n",
        "train_data = {}\n",
        "validation_data = {}\n",
        "test_data = {}\n",
        "\n",
        "for category, files in all_files.items():\n",
        "    total_files = len(files)\n",
        "    train_split = int(train_ratio * total_files)\n",
        "    validation_split = int(validation_ratio * total_files)\n",
        "\n",
        "    train_data[category] = files[:train_split]\n",
        "    validation_data[category] = files[train_split:train_split+validation_split]\n",
        "    test_data[category] = files[train_split+validation_split:]\n",
        "\n",
        "# Fungsi untuk memindahkan file ke direktori output\n",
        "def move_files(files, source_path, destination_path):\n",
        "    for file in files:\n",
        "        source_file = os.path.join(source_path, file)\n",
        "        destination_file = os.path.join(destination_path, file)\n",
        "        shutil.copyfile(source_file, destination_file)\n",
        "\n",
        "# Memindahkan file ke direktori output\n",
        "for category, files in train_data.items():\n",
        "    source_path = os.path.join(dataset_path, category)\n",
        "    destination_path = os.path.join(output_path, \"train\", category)\n",
        "    os.makedirs(destination_path, exist_ok=True)\n",
        "    move_files(files, source_path, destination_path)\n",
        "\n",
        "for category, files in validation_data.items():\n",
        "    source_path = os.path.join(dataset_path, category)\n",
        "    destination_path = os.path.join(output_path, \"validation\", category)\n",
        "    os.makedirs(destination_path, exist_ok=True)\n",
        "    move_files(files, source_path, destination_path)\n",
        "\n",
        "for category, files in test_data.items():\n",
        "    source_path = os.path.join(dataset_path, category)\n",
        "    destination_path = os.path.join(output_path, \"test\", category)\n",
        "    os.makedirs(destination_path, exist_ok=True)\n",
        "    move_files(files, source_path, destination_path)\n",
        "print(\"Split Data Selesai\")"
      ],
      "metadata": {
        "id": "cBfSNcLmg4iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "train_folder = \"/content/drive/MyDrive/DatasetSplit/train\"\n",
        "validation_folder = \"/content/drive/MyDrive/DatasetSplit/validation\"\n",
        "test_folder = \"/content/drive/MyDrive/DatasetSplit/test\"\n",
        "\n",
        "categories = [\"Mass\", \"Nodule\", \"Normal\", \"Pneumonia\", \"Tuberculosis\"]\n",
        "\n",
        "print(\"TRAIN\")\n",
        "for category in categories:\n",
        "    category_folder = os.path.join(train_folder, category)\n",
        "    file_count = len(os.listdir(category_folder))\n",
        "    print(f\"Jumlah file pada folder {category}: {file_count}\")\n",
        "print(\"VALIDATION\")\n",
        "for category in categories:\n",
        "    category_folder = os.path.join(validation_folder, category)\n",
        "    file_count = len(os.listdir(category_folder))\n",
        "    print(f\"Jumlah file pada folder {category}: {file_count}\")\n",
        "print(\"TEST\")\n",
        "for category in categories:\n",
        "    category_folder = os.path.join(test_folder, category)\n",
        "    file_count = len(os.listdir(category_folder))\n",
        "    print(f\"Jumlah file pada folder {category}: {file_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCgaIC9mhB64",
        "outputId": "b9cf0ca2-4ccd-4795-c254-c4a14d770a32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN\n",
            "Jumlah file pada folder Mass: 1788\n",
            "Jumlah file pada folder Nodule: 1800\n",
            "Jumlah file pada folder Normal: 1800\n",
            "Jumlah file pada folder Pneumonia: 1786\n",
            "Jumlah file pada folder Tuberculosis: 1800\n",
            "VALIDATION\n",
            "Jumlah file pada folder Mass: 100\n",
            "Jumlah file pada folder Nodule: 100\n",
            "Jumlah file pada folder Normal: 100\n",
            "Jumlah file pada folder Pneumonia: 100\n",
            "Jumlah file pada folder Tuberculosis: 100\n",
            "TEST\n",
            "Jumlah file pada folder Mass: 100\n",
            "Jumlah file pada folder Nodule: 100\n",
            "Jumlah file pada folder Normal: 100\n",
            "Jumlah file pada folder Pneumonia: 100\n",
            "Jumlah file pada folder Tuberculosis: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**MobileNet**"
      ],
      "metadata": {
        "id": "8iOIS7oUmizn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNet\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Tentukan path dataset split\n",
        "train_dir = \"/content/drive/MyDrive/DatasetSplit/train\"\n",
        "validation_dir = \"/content/drive/MyDrive/DatasetSplit/validation\"\n",
        "test_dir = \"/content/drive/MyDrive/DatasetSplit/test\"\n",
        "\n",
        "# Tentukan parameter model\n",
        "input_shape = (224, 224)\n",
        "num_classes = 5\n",
        "batch_size = 64\n",
        "epochs = 50\n",
        "\n",
        "# Preprocess data train\n",
        "train_datagen = ImageDataGenerator(\n",
        "    # rescale=1.0/255.0,\n",
        "    # shear_range=0.2,\n",
        "    # zoom_range=0.2,\n",
        "    # horizontal_flip=True\n",
        "    )\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=input_shape,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        "    )\n",
        "\n",
        "# Preprocess data validasi\n",
        "validation_datagen = ImageDataGenerator(\n",
        "    # rescale=1.0/255.0\n",
        "    )\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=input_shape,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    )\n",
        "\n",
        "\n",
        "# Load model MobileNet tanpa bagian fully connected terakhir (top)\n",
        "base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(input_shape[0], input_shape[1], 3))\n",
        "\n",
        "# Tetapkan lapisan base_model agar tidak dilatih\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Tambahkan fully connected layer baru\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "# Buat model baru dengan arsitektur yang telah diubah\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Latih model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // batch_size\n",
        "    )\n",
        "\n",
        "# Test Generator\n",
        "test_datagen = ImageDataGenerator()\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=input_shape,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False)\n",
        "\n",
        "# Prediksi data test\n",
        "y_pred = model.predict(test_generator)\n",
        "\n",
        "# Prediksi berdasaarkan class/label\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "class_names = [\"Mass\", \"Nodule\", \"Normal\", \"Pneumonia\", \"Tuberculosis\"]\n",
        "\n",
        "# get true labels from the generator\n",
        "y_true = test_generator.classes\n",
        "\n",
        "# Confusion Matrix\n",
        "report = classification_report(y_true, y_pred_labels, target_names=class_names, digits=5)\n",
        "conf_mat = confusion_matrix(y_true, y_pred_labels)\n",
        "\n",
        "print(report)\n",
        "print(conf_mat)\n",
        "\n",
        "# Load batch data\n",
        "batch_images, batch_labels = next(test_generator)\n",
        "\n",
        "# Predict labels/class dengan model\n",
        "predicted_labels = model.predict(batch_images)\n",
        "\n",
        "# Daftar class\n",
        "class_names = list(train_generator.class_indices.keys())\n",
        "\n",
        "# Membuat plot\n",
        "grid_size = (2, 4)\n",
        "fig, ax = plt.subplots(nrows=grid_size[0], ncols=grid_size[1], figsize=(10, 10))\n",
        "for i , axi in enumerate(ax.flat):\n",
        "    # Plot image\n",
        "    axi.imshow(batch_images[i])\n",
        "    axi.set_title(f\"True: {class_names[np.argmax(batch_labels[i])]}\\nPredicted: {class_names[np.argmax(predicted_labels[i])]}\")\n",
        "    axi.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(conf_mat, annot=True, cmap='Blues', fmt='g', \n",
        "            xticklabels=[\"Mass\", \"Nodule\", \"Normal\", \"Pneumonia\", \"Tuberculosis\"], \n",
        "            yticklabels=[\"Mass\", \"Nodule\", \"Normal\", \"Pneumonia\", \"Tuberculosis\"])\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()\n",
        "\n",
        "# Mengambil nilai loss dan akurasi dari history\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "train_acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "# Membuat grafik loss\n",
        "plt.plot(train_loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Membuat grafik akurasi\n",
        "plt.plot(train_acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Simpan model\n",
        "model.save(\"/content/drive/MyDrive/MobileNet_model.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYXO22wqhCeM",
        "outputId": "602c0504-fa0a-4aab-c471-ce9f0d115967"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8918 images belonging to 5 classes.\n",
            "Found 500 images belonging to 5 classes.\n",
            "Epoch 1/50\n",
            " 18/139 [==>...........................] - ETA: 8:00 - loss: 1.8513 - accuracy: 0.4459"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sH0K0-haoa1u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}